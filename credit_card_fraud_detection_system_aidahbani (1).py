# -*- coding: utf-8 -*-
"""Credit card Fraud Detection system AIDAHBANI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FneLMC2IDYUdn3qwTX6UoLAwNr978Qln
"""



"""# ***Credit Card Fraud Detection***
Goal:
Try to detect whether credit card transaction is fraudalent or not based on certain predictors.

Attribute Information:
distance_from_home - the distance from home where the transaction happened.

distance_from_last_transaction - the distance from last transaction happened.

ratio_to_median_purchase_price - Ratio of purchased price transaction to median purchase price.

repeat_retailer - Is the transaction happened from same retailer.

used_chip - Is the transaction through chip (credit card).

used_pin_number - Is the transaction happened by using PIN number.

online_order - Is the transaction an online order.

fraud - Is the transaction fraudulent. (1 represents a fraudulent transaction)

"""

# Commented out IPython magic to ensure Python compatibility.
# import essential libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# load the data
card_url = "https://drive.google.com/file/d/1XdIqSj2n4fZ_B2lLyWNtBisDLDScxSd8/view?usp=drive_link"
card_path = "https://drive.google.com/uc?id=" + card_url.split("/")[-2]
card = pd.read_csv(card_path)

# create a copy of data frame to keep original
real_card = card.copy()
card.head()

# data dimensions
card.shape

# column names
card.columns

# column information
card.info()

card.describe().T

# check for missing values
card.isna().sum()

"""**Visualizations**"""

# check for correlation between variables
cor = card.corr()
sns.heatmap(cor, annot = True, fmt = '.2f', cmap = 'crest', linewidth = 0.7)
plt.title("Correlation Matrix")
plt.show()

"""Looking at the correlation heatap above, we can see we have a strong correlation between our target variable fraud and the ratio to median purchase median price feature. Therefore, we must have ratio to median purchase price as a predictor for our models."""

# check the distribution of class on our target variable
card['fraud'].value_counts()
sns.countplot(x = 'fraud', data = card, palette = 'magma')
plt.title("Fraud Classes Count")
plt.xticks(ticks = [0,1], labels = ['Legitimate','Fraud'])
plt.xlabel('Fraud or Not')
plt.show()

"""We can clearly see the distribution between the number of fraudlent purchases and non-fraudlent purchase where there is a significant amount of non-fraudulent purchases in our data."""

# check if for fraud occurrences in online purchases
sns.countplot(x = 'online_order',hue = 'fraud', data = card, palette = 'magma')
print(pd.crosstab(index = card['fraud'], columns = card['online_order']))
plt.title("fraudulent Transaction Dependent on Online Purchases")
plt.xticks(ticks = [0,1], labels = ['No','Yes'])
plt.show()

"""As we can see there was definitely more fraud purchases done online compared to purchases not done online."""

# check if for fraud occurrences if the card pin was used
sns.countplot(x = 'used_pin_number',hue = 'fraud', data = card, palette = 'magma')
print(pd.crosstab(index = card['fraud'], columns = card['used_pin_number']))
plt.title("fraudulent Transaction Dependent on Pin Number Usage")
plt.xticks(ticks = [0,1], labels = ['No','Yes'])
plt.show()

"""Now comparing fraudulent purchase where the card pin was used we can see a majority of fraudulent purchases occured without using the card pin."""

# check if for fraud occurrences if the card chip was used
sns.countplot(x = 'used_chip',hue = 'fraud', data = card, palette = 'magma')
plt.title("fraudulent Transaction Dependent on Used Chip")
print(pd.crosstab(index = card['fraud'], columns = card['used_chip']))
plt.xticks(ticks = [0,1], labels = ['No','Yes'])
plt.show()

"""There was more fraudulent purchases where the chip on the card was not used."""

# check if for fraud occurrences if it was in a repeated retailer
sns.countplot(x = 'repeat_retailer',hue = 'fraud', data = card, palette = 'magma')
print(pd.crosstab(index = card['fraud'], columns = card['repeat_retailer']))
plt.title("fraudulent Transaction Dependent on Repeated Retailer")
plt.xticks(ticks = [0,1], labels = ['No','Yes'])
plt.show()

"""This one was interesting to me since more fraudulent purchases occured where the card was used to purchase an item previously. The probability of a fraudulent purchase in a repeated retailer was greater than a purchase on a unknown location. I don't see this as a coincidence or perhaps the retailer is a common place where fraudster go and buy items."""

sns.boxplot(x = 'fraud', y = 'distance_from_last_transaction',
                data = card, palette = 'Set2')
plt.title("fraudulent Transactions by Distance of Last Transaction")
plt.show()

"""This one was also interesting to me, where fraudulent purchases occured relatively close to the lastest transaction on the same card."""

sns.boxplot(x = 'fraud', y = 'distance_from_home',
                data = card, palette = 'Set2')
plt.title("fraudulent Transactions by Distance from Home")
plt.show()

"""Preprocessing for Modeling
We have to be careful the way we split the data since we observed there were a huge difference between legitimate purchases and fraudulent purchases, there were way more legitimate purchases in our dataset. We want to split the data so that when we train the model so that it has a good amount of samples to learn normal purchases but also enough samples to test our model to check whether a observation was fraudulent. To solve this issue we will use a specific strategy called stratified sampling where instead of pulling a certain subset of the data we will pull a shuffled sampling from the data. Note: This does not solve the problem of class imbalance in our dataset, however, it provides a better method of splitting the data
"""

# metrics we will use to find the best model
from sklearn.metrics import f1_score,accuracy_score,recall_score,precision_score,confusion_matrix, classification_report

# convert to integer data type since they are categorical variables
card[['repeat_retailer','used_chip','used_pin_number','online_order','fraud']] = card[['repeat_retailer','used_chip','used_pin_number','online_order','fraud']].astype('int')

"""Split the Data"""

# split the data

# our predictor variables
X = card.drop(['fraud'], axis = 1)

# our target variable
y = card['fraud']

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold

# splitting our data into training and testing sets
# we startify our target variable
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.30, random_state = 42, stratify = y)

# training sets shape
X_train.shape, y_train.shape

# testing sets shape
X_test.shape, y_test.shape

X_train

y_train.value_counts()

y_test.value_counts()

"""Modeling :
Random Forest Classifier
"""

# import our random forest classifier
from sklearn.ensemble import RandomForestClassifier

# instantiate our model
rfc = RandomForestClassifier(random_state = 0)
# train our model
rfc.fit(X_train, y_train)

# stratified shuffle
skf = StratifiedKFold(shuffle = True, n_splits = 5, random_state = 5)

# cross Validation scores
tr_sc = cross_val_score(rfc, X_train, y_train, cv = skf)
te_sc = cross_val_score(rfc, X_test, y_test, cv = skf)

print('Random Forest Classifier scores:')
print(f'Mean accuracy score of 5-fold CV on Training set: {tr_sc.mean()} (std: {tr_sc.std()})')
print(f'Mean accuracy score of 5-fold CV on Testing set: {te_sc.mean()} (std: {te_sc.std()})')

# create a feature importance data frame
dict1 = {'columns': rfc.feature_names_in_, 'importance': rfc.feature_importances_}
forest_importances = pd.DataFrame(dict1).sort_values('importance', ascending = False)
forest_importances

# plotting a horizontal bar chart on feature importance
sns.barplot(data = forest_importances, y = 'columns', x = 'importance')
plt.title("Random Forest Feature Importance")
plt.show()

# import our xgboost classifier
from xgboost import XGBClassifier

# instantiate our model
boost = XGBClassifier(random_state = 5)
# train our boosting model
boost.fit(X_train,y_train)

# our stratified shuffle
skf = StratifiedKFold(shuffle = True, n_splits = 5, random_state = 5)
# 5-fold cross validation
tr_sc = cross_val_score(boost, X_train, y_train, cv = skf)
te_sc = cross_val_score(boost, X_test, y_test, cv = skf)

# accuracy mean score
print('XGBoost Classifier scores:')
print(f'Mean accuracy score of 5-fold CV on Training set: {tr_sc.mean()} (std: {tr_sc.std()})')
print(f'Mean accuracy score of 5-fold CV on Testing set: {te_sc.mean()} (std: {te_sc.std()})')

# create a feature importance data frame
dict1 = {'columns': boost.feature_names_in_, 'importance': boost.feature_importances_}
boost_importances = pd.DataFrame(dict1).sort_values('importance', ascending = False)
boost_importances

# plotting a horizontal bar chart on feature importance
sns.barplot(data = boost_importances, y = 'columns', x = 'importance')
plt.title("XGBClassifier Feature Importance")
plt.show()

"""**Logistic Regression**"""

# import our logistic regression
from sklearn.linear_model import LogisticRegression

# Logistic Regression with Lasso penalty (L1)
log_l1 = LogisticRegression(solver = 'liblinear',penalty = 'l1')
log_l1.fit(X_train,y_train)

# stratified shuffle
skf = StratifiedKFold(shuffle = True, n_splits = 5, random_state = 5)
tr_sc = cross_val_score(log_l1, X_train, y_train, cv = skf)
te_sc = cross_val_score(log_l1, X_test, y_test, cv = skf)

# accuracy mean scores
print('Logistic Regression with L1 penalty, scores:')
print(f'Mean accuracy score of 5-fold CV on Training set: {tr_sc.mean()} (std: {tr_sc.std()})')
print(f'Mean accuracy score of 5-fold CV on Testing set: {te_sc.mean()} (std: {te_sc.std()})')

# Logistic Regression With Ridge penalty (L2)
log_l2 = LogisticRegression(solver = 'liblinear',penalty = 'l2')
log_l2.fit(X_train,y_train)

skf = StratifiedKFold(shuffle = True, n_splits = 5, random_state = 5)
tr_sc = cross_val_score(log_l2, X_train, y_train, cv = skf)
te_sc = cross_val_score(log_l2, X_test, y_test, cv = skf)


print('Logistic Regression with L2 penalty, scores:')
print(f'Mean accuracy score of 5-fold CV on Training set: {tr_sc.mean()} (std: {tr_sc.std()})')
print(f'Mean accuracy score of 5-fold CV on Testing set: {te_sc.mean()} (std: {te_sc.std()})')

# L1 penalty performed slightly better will make a prediction with that model
y_pred = log_l1.predict(X_test)

# Confusion Matrix
log_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(log_matrix, annot=True, fmt="d")
plt.title('Logistic Regression L1 penalty Confusion Matrix')
plt.xlabel('Actual')
plt.ylabel('Predicted')

print(f"The accuracy of Logistic(L1) model on testing set is: {accuracy_score(y_test, y_pred)*100}% \n")

# logistic regression with lasso penalty (L1) Classification Report
print(classification_report(y_test, y_pred))

"""Creating a confusion matrix on the testing set and using sklearn confusion matrix documentation the upper left corner represents the true negatives. In our scenario the true negatives represents all non-fraudulent purchase predicted accurately by the Logistic Regression model. The lower right corner are the true positives or the fraudulent purchases predicted accurately by our model. However in the top right corner we have the false positives, which represents all the observations categorized as fraudulent purchases but actually weren't fraudulent purchases. The number of false positives in our model was about 1841 purchases that were considered fraudulent transactions but actually were not. Now this represents a problem since we are denying a good amount of purchases, since they were considered fraudulent but weren't, this mistake might cost a company a huge amount of money. So we want to reduce the amount of false positives and categorized them as verified purchases. Note we might classify actual fraudulent purchases as non-fraudulent purchases but this scenario would cost the company less expense rather than considering worst case scenario as a fraudulent purchase."""

log_l1.predict_proba(X_test)

# will change the probability threshold for predicting a fraudulent transaction
THRESHOLD = .70
y_pred = np.where(log_l1.predict_proba(X_test)[:,1] > THRESHOLD, 1, 0)

# Confusion Matrix
log_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(log_matrix, annot=True, fmt="d")
plt.title('Logistic Regression L1 penalty Confusion Matrix')
plt.xlabel('Actual')
plt.ylabel('Predicted')

print(f"The accuracy of Logistic(L1) model on testing set is: {accuracy_score(y_test, y_pred)*100}% \n")

"""Observe how increasing the threshold for a fraudulent purchase probability to be greater than 75%, we were able to reduce the number of false positives by 700. However, this came at a cost of reducing the number of true positive, the number of predicting a fraudulent purchase. We also increased the number of false negatives, the fraudulent purchases classified as non-fraudulent purchases.

# **Finding best Threshold**
We have to consider we want to find the best model that has the highest prediction for fraudulent transactions while having a relatively low error on misclassified fraudulent transactions. In other words we want our true positives to be high and false positives to be low in our confusion matrix. The reason we want this is that it can cost the company a good amount of money to by misidentify a fraudulent transaction.
"""

# Define a function to find the best threshold for a given model
def find_best_threshold(model, num_steps):
    highest_f1 = 0
    best_threshold = 0
    best_acc = 0
    best_rec = 0
    best_pre = 0
    # Iterate over a range of thresholds
    for threshold in np.linspace(0, 1, num_steps):
        # Predict the target variable using the given threshold
        y_predict = (model.predict_proba(X_test)[:, 1] >= threshold)
        # Calculate various evaluation metrics
        f1 = f1_score(y_test, y_predict)
        acc = accuracy_score(y_test, y_predict)
        rec = recall_score(y_test, y_predict)
        pre = precision_score(y_test, y_predict)
        # Update the best threshold and metrics if F1 score improves
        if f1 > highest_f1:
            best_threshold, highest_f1, best_acc, best_rec, best_pre = \
                threshold, f1, acc, rec, pre
    # Return the best threshold and evaluation metrics
    return best_threshold, highest_f1, best_acc, best_rec, best_pre

# first we define our models and objects into a list
model_names = ["Logistic Regression", "Random Forest", "XGBClassifier"]
models = [log_l1, rfc, boost]

# Create an empty list to store the results
chart = list()

# Iterate over the models and find the best threshold for each one
for item, name in zip(models, model_names):
    best_thresh, high_f1, high_acc, high_rec, high_pre = find_best_threshold(item, 20)
    # Append the results to the chart list
    chart.append([name, best_thresh, high_f1, high_acc, high_rec, high_pre])

# Create a pandas dataframe from the chart list and display it
chart = pd.DataFrame(chart, columns=['Model', 'Best Threshold', 'F1 Score', 'Accuracy', 'Recall', 'Precision'])
#chart.to_csv('model_evaluation_scores.csv')
chart

def make_confusion_matrix_val(model, threshold=0.5):
    """
    Create a confusion matrix plot for the given model and threshold.

    Parameters:
    -----------
    model : sklearn classifier
        The classification model to evaluate.
    threshold : float, default=0.5
        Probability threshold for binary classification.

    Returns:
    --------
    None

    """
    # Predict class 1 if probability of being in class 1 is greater than threshold
    # (model.predict(X_test) does this automatically with a threshold of 0.5)
    y_predict = (model.predict_proba(X_test)[:, 1] >= threshold)

    # calculate the confusion matrix
    fraud_confusion = confusion_matrix(y_test, y_predict)

    # plot the confusion matrix as heatmap
    plt.figure(dpi=100)
    sns.set(font_scale=1)
    sns.heatmap(fraud_confusion, cmap=plt.cm.Blues, annot=True, square=True, fmt='d',
            xticklabels=['Not Fraud', 'Fraud'],
            yticklabels=['Not Fraud', 'Fraud'])

    # calculate TP, FP, FN, and TN values from the confusion matrix
    TP = fraud_confusion[0][0]
    FP = fraud_confusion[0][1]
    FN = fraud_confusion[1][0]
    TN = fraud_confusion[1][1]

    # rotate y-axis ticks
    plt.yticks(rotation = 0)

    # set plot title, x and y labels
    plt.title('Predicted vs. Actual', fontsize = 20, pad = 10)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')

# Logistic regression confusion matrix with best threshold
make_confusion_matrix_val(log_l1, threshold=0.315789)

# XGBoost confusion matrix with best threshold
make_confusion_matrix_val(boost, threshold=0.105263)

"""# **Credit Card Fraud Predictor System**"""

real_card.loc[real_card['fraud']==1]

# we choose a random observation
sample = real_card.sample(1)

# check if the example is a fraudulent purchase or not
sample.T

"""Lets see if our credit card fraud detector system is able to predict the status of this transaction."""

# we drop the fraud column from our sample
example = sample.drop(['fraud'], axis = 1)

# check if fraud column from sample was dropped
example.head()

def preprocess(data):
  # convert data type
  data[['repeat_retailer','used_chip','used_pin_number','online_order']] = data[['repeat_retailer','used_chip','used_pin_number','online_order']].astype('int')

  # scale some columns
  # data[['distance_from_home','distance_from_last_transaction','ratio_to_median_purchase_price']] = scaler.transform(data[['distance_from_home','distance_from_last_transaction','ratio_to_median_purchase_price']])

  return data

# credit card fraud detection system

def fraud_prediction(obs):
    obs = preprocess(obs)
    result = np.where((boost.predict_proba(obs)[:, 1] >= 0.105263),1,0)
    if (result == 1):
        return "This is a fraudulent purchase!"
    else:
        return "This transaction is verified"

# check the results
fraud_prediction(example)

"""# **Deployment of Model**"""

# import shap library
import shap

"""SHAP (SHapley Additive exPlanations) is a method used for explaining the output of machine learning models. It provides a way to understand the contribution of individual features to the prediction made by the model. SHAP values are based on cooperative game theory and provide a fair way to allocate the "credit" of the model's prediction to each feature.

Here's a brief overview of how SHAP feature importance works:

Background Dataset: To calculate SHAP values, a background dataset is required. This dataset represents the "average" or "baseline" state of the features and is used as a reference point to measure the impact of individual features on predictions.

Model Prediction: For each instance in the dataset you want to explain, the model predicts an output (e.g., a probability score or a regression value).

SHAP Values Calculation: SHAP values are computed by running the model on all possible combinations of features (subsets) for each instance. The contribution of each feature to the prediction is determined by comparing the model's output when the feature is included or excluded.

Shapley Values: SHAP values are calculated using the concept of Shapley values from cooperative game theory. These values fairly distribute the model's prediction among the individual features.

Feature Importance: After calculating SHAP values for all instances, you can aggregate them to get feature importances. This can be done by averaging the absolute SHAP values for each feature across the entire dataset or by using other summary statistics.

Interpretation: Positive SHAP values indicate that the feature pushes the model's prediction higher, while negative values indicate the opposite. The magnitude of the SHAP value represents the strength of the feature's impact.
"""

# shap values for an example observation
# this cell is to prepare a shap importance plot for our web app
plt.style.use("fivethirtyeight")

explainer = shap.TreeExplainer(boost)
shap_values = explainer.shap_values(example)
scores_desc = list(zip(shap_values[0], example.columns))
scores_desc = sorted(scores_desc)
fig_m = plt.figure(tight_layout=True)
plt.barh([s[1] for s in scores_desc], [s[0] for s in scores_desc])
plt.title("Feature Shap Values")
plt.ylabel("Shap Value")
plt.xlabel("Feature Importance")
plt.tight_layout()

def predict(*data):
      columns = ['distance_from_home', 'distance_from_last_transaction',
            'ratio_to_median_purchase_price', 'repeat_retailer', 'used_chip',
            'used_pin_number', 'online_order']
      df = pd.DataFrame([data], columns = columns)
      df = preprocess(df)
      prob_pred = boost.predict_proba(df)
      return {"Normal": float(prob_pred[0][0]), "Fraud": float(prob_pred[0][1])}

def interpret(*data):

      plt.style.use("fivethirtyeight")

      columns = ['distance_from_home', 'distance_from_last_transaction',
            'ratio_to_median_purchase_price', 'repeat_retailer', 'used_chip',
            'used_pin_number', 'online_order']
      df = pd.DataFrame([data], columns = columns)

      explainer = shap.TreeExplainer(boost)
      shap_values = explainer.shap_values(df)
      scores_desc = list(zip(shap_values[0], df.columns))
      scores_desc = sorted(scores_desc)
      fig_m = plt.figure(tight_layout=True)
      plt.barh([s[1] for s in scores_desc], [s[0] for s in scores_desc])
      plt.title("Feature Shap Values")
      plt.ylabel("Shap Value")
      plt.xlabel("Feature Importance")
      plt.tight_layout()
      return fig_m

"""**Conclusion**:
The XGBoost Classifier algorithm returned an accuracy score on both the training and testing data of above 99.99%. The accuracy score recieved was the mean of a 5-fold stratified cross validation on both data sets. Therefore, we can conclude that the model was not over fitting or being biased by the data it was trained on. This makes the credit card fraud detection system a pretty accurate detector for fraudulent transactions
"""

import streamlit as st

st.title("Credit Card Fraud Prediction System")
st.write("""
This is a Web App that predicts whether a credit card transaction is fraudulent or not. Just input the following parameters and click the predict button. If you want to see the influence that each parameter had on the outcome, click the explain button.
""")

# Input parameters
repeated_retailer = st.radio("Repeat Retailer", ["No", "Yes"], index=0, help="Was the transaction at a repeated store?")
online_order = st.radio("Online Order", ["No", "Yes"], index=0, help="Was the transaction an online order?")
used_chip = st.radio("Used Chip", ["No", "Yes"], index=0, help="Did the purchase use the security chip of the card?")
used_pin = st.radio("Used Pin Number", ["No", "Yes"], index=0, help="Did the transaction use the pin code of the card?")
distance_home = st.number_input("Distance From Home (miles)", value=25, help="How far was the transaction from the card owner's house? (in miles)")
distance_last = st.number_input("Distance From Last Transaction (miles)", value=5, help="How far away was it from the last transaction that happened? (in miles)")
ratio_median = st.number_input("Ratio Median Purchase Price", value=1.8, help="Divide the purchase price by card owner's median purchase price")

# Prediction and Explanation buttons
predict_btn = st.button("Predict")
interpret_btn = st.button("Explain")

# Dummy prediction and interpretation functions
def predict(inputs):
    # Replace with your model's prediction code
    return "Fraudulent" if sum(inputs) > 50 else "Not Fraudulent"

def interpret(inputs):
    # Replace with your model's interpretation code
    import matplotlib.pyplot as plt
    fig, ax = plt.subplots()
    ax.bar(["Distance Home", "Distance Last", "Ratio Median"], inputs[:3])
    return fig

# Handling button clicks
if predict_btn:
    inputs = [distance_home, distance_last, ratio_median, repeated_retailer, used_chip, used_pin, online_order]
    inputs = [float(val) if isinstance(val, (int, float)) else 1 if val == "Yes" else 0 for val in inputs]
    prediction = predict(inputs)
    st.subheader("Prediction")
    st.write(prediction)

if interpret_btn:
    inputs = [distance_home, distance_last, ratio_median, repeated_retailer, used_chip, used_pin, online_order]
    inputs = [float(val) if isinstance(val, (int, float)) else 1 if val == "Yes" else 0 for val in inputs]
    fig = interpret(inputs)
    st.subheader("Interpretation")
    st.pyplot(fig)

"""Running on Url http://localhost:8501

"""

example.T

real_card.loc[real_card.index == 13].T